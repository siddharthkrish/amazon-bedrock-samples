{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Bedrock Service Tier Performance Comparison\n",
    "\n",
    "This notebook compares the performance of Amazon Bedrock's Priority, Standard (default), and Flex tiers for a given model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from typing import Dict, List\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock = boto3.client('bedrock-runtime', region_name='us-west-2')\n",
    "\n",
    "NOVA_MODEL = 'us.amazon.nova-pro-v1:0'\n",
    "MODEL_ID = 'moonshot.kimi-k2-thinking'\n",
    "TIERS = ['priority', 'default', 'flex']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Test Prompts\n",
    "\n",
    "Using Amazon Nova Pro to generate prompts of varying lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompts(count: int = 60) -> List[str]:\n",
    "    prompt = f\"\"\"Generate {count} diverse prompts of varying lengths for testing an AI model. \n",
    "    Include:\n",
    "    - 20 short prompts (10-30 words)\n",
    "    - 20 medium prompts (50-150 words)\n",
    "    - 20 long prompts (200-400 words)\n",
    "    \n",
    "    Topics should vary: technical questions, creative writing, analysis, coding, explanations.\n",
    "    Return ONLY a JSON array of strings, no other text.\"\"\"\n",
    "    \n",
    "    response = bedrock.converse(\n",
    "        modelId=NOVA_MODEL,\n",
    "        messages=[{\"role\": \"user\", \"content\": [{\"text\": prompt}]}]\n",
    "    )\n",
    "    \n",
    "    text = response['output']['message']['content'][0]['text']\n",
    "    return json.loads(text)\n",
    "\n",
    "prompts = generate_prompts()\n",
    "print(f\"Generated {len(prompts)} prompts\")\n",
    "print(f\"Sample prompt lengths: {[len(p.split()) for p in prompts[:5]]} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Function\n",
    "\n",
    "Sends prompts to each tier and collects performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_tier(prompt: str, tier: str) -> Dict:\n",
    "    start = time.time()\n",
    "    \n",
    "    response = bedrock.converse_stream(\n",
    "        modelId=MODEL_ID,\n",
    "        messages=[{\"role\": \"user\", \"content\": [{\"text\": prompt}]}],\n",
    "        serviceTier={\"type\": tier}\n",
    "    )\n",
    "    \n",
    "    first_token_time = None\n",
    "    tokens = 0\n",
    "    \n",
    "    for event in response['stream']:\n",
    "        if 'contentBlockDelta' in event:\n",
    "            if first_token_time is None:\n",
    "                first_token_time = time.time()\n",
    "            tokens += 1\n",
    "        elif 'metadata' in event:\n",
    "            usage = event['metadata'].get('usage', {})\n",
    "            input_tokens = usage.get('inputTokens', 0)\n",
    "            output_tokens = usage.get('outputTokens', 0)\n",
    "    \n",
    "    end = time.time()\n",
    "    \n",
    "    return {\n",
    "        'tier': tier,\n",
    "        'input_tokens': input_tokens,\n",
    "        'output_tokens': output_tokens,\n",
    "        'time_to_first_token': first_token_time - start if first_token_time else None,\n",
    "        'time_to_last_token': end - start,\n",
    "        'throughput': output_tokens / (end - start) if end > start else 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Tests\n",
    "\n",
    "Testing all prompts across all tiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for i, prompt in enumerate(prompts):\n",
    "    print(f\"Testing prompt {i+1}/{len(prompts)}...\")\n",
    "    for tier in TIERS:\n",
    "        try:\n",
    "            result = test_tier(prompt, tier)\n",
    "            result['prompt_id'] = i\n",
    "            result['prompt_length'] = len(prompt.split())\n",
    "            results.append(result)\n",
    "            time.sleep(0.5)\n",
    "        except Exception as e:\n",
    "            print(f\"Error with tier {tier}, prompt {i}: {e}\")\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "print(f\"\\nCompleted {len(results)} tests\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = df.groupby('tier').agg({\n",
    "    'time_to_first_token': ['mean', 'median', 'std', 'min', 'max'],\n",
    "    'time_to_last_token': ['mean', 'median', 'std', 'min', 'max'],\n",
    "    'throughput': ['mean', 'median', 'std'],\n",
    "    'input_tokens': 'mean',\n",
    "    'output_tokens': 'mean'\n",
    "}).round(4)\n",
    "\n",
    "summary.columns = ['_'.join(col).strip() for col in summary.columns.values]\n",
    "summary = summary.rename(columns={\n",
    "    'time_to_first_token_mean': 'TTFT_mean (s)',\n",
    "    'time_to_first_token_median': 'TTFT_median (s)',\n",
    "    'time_to_first_token_std': 'TTFT_std (s)',\n",
    "    'time_to_first_token_min': 'TTFT_min (s)',\n",
    "    'time_to_first_token_max': 'TTFT_max (s)',\n",
    "    'time_to_last_token_mean': 'TTLT_mean (s)',\n",
    "    'time_to_last_token_median': 'TTLT_median (s)',\n",
    "    'time_to_last_token_std': 'TTLT_std (s)',\n",
    "    'time_to_last_token_min': 'TTLT_min (s)',\n",
    "    'time_to_last_token_max': 'TTLT_max (s)',\n",
    "    'throughput_mean': 'Throughput_mean (tok/s)',\n",
    "    'throughput_median': 'Throughput_median (tok/s)',\n",
    "    'throughput_std': 'Throughput_std (tok/s)',\n",
    "    'input_tokens_mean': 'Avg_input_tokens',\n",
    "    'output_tokens_mean': 'Avg_output_tokens'\n",
    "})\n",
    "\n",
    "print(\"\\n=== PERFORMANCE COMPARISON BY TIER ===\")\n",
    "print(summary.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== KEY METRICS COMPARISON ===\")\n",
    "key_metrics = df.groupby('tier')[['time_to_first_token', 'time_to_last_token', 'throughput']].mean().round(4)\n",
    "key_metrics.columns = ['Avg TTFT (s)', 'Avg TTLT (s)', 'Avg Throughput (tok/s)']\n",
    "print(key_metrics.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance by Input Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['size_category'] = pd.cut(df['prompt_length'], bins=[0, 50, 150, 500], labels=['Short', 'Medium', 'Long'])\n",
    "\n",
    "size_analysis = df.groupby(['tier', 'size_category']).agg({\n",
    "    'time_to_first_token': 'mean',\n",
    "    'throughput': 'mean'\n",
    "}).round(4)\n",
    "\n",
    "print(\"\\n=== PERFORMANCE BY INPUT SIZE ===\")\n",
    "print(size_analysis.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run this if you want to save the results in a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('bedrock_tier_results.csv', index=False)\n",
    "print(\"\\nResults saved to bedrock_tier_results.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
